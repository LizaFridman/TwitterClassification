{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"gender-classifier-DFE-791531.csv\"\n",
    "\n",
    "loaded_tweets = pd.read_csv(path_to_file, encoding='latin-1')\n",
    "\n",
    "genders = {'female', 'male', 'brand'}\n",
    "clean_loaded_tweets = loaded_tweets.loc[loaded_tweets['gender'].isin(genders)]\n",
    "\n",
    "clean_loaded_tweets.loc[loaded_tweets['text'].isnull(), 'text'] = \"-\"\n",
    "clean_loaded_tweets.loc[loaded_tweets['description'].isnull(), 'description'] = \"-\"\n",
    "\n",
    "tweets = clean_loaded_tweets[\"text\"].map(str) + \" \" + clean_loaded_tweets[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loaded Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female     6700\n",
      "male       6194\n",
      "brand      5942\n",
      "unknown    1117\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaned Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female    6700\n",
      "male      6194\n",
      "brand     5942\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(clean_loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop_words = set(stopwords.words('english') + punctuation + list(\"__\"))\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    tokenized_clean_text = []\n",
    "    text = re.sub(r'[^\\x00-\\x7f]*', r'', text)\n",
    "    tokens = tokens_re.findall(text)\n",
    "    tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        tokenized_clean_text.append(token)\n",
    "    return tokenized_clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweet = clean_and_tokenize(tweet)\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "        \n",
    "gender_column = clean_loaded_tweets[\"gender\"]\n",
    "counter = Counter(gender_column)\n",
    "genders = counter.keys()\n",
    "tweet_amount = counter.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Distributions By Gender\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes = np.arange(len(genders))\n",
    "plt.bar(indexes, tweet_amount, 0.5)\n",
    "plt.xticks(indexes + 0.5 * 0.5, genders)\n",
    "print('Tweet Distribution By Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_most_common_terms(terms,top):\n",
    "    res = []\n",
    "    counter = Counter()\n",
    "    for term in terms:\n",
    "        res.extend([token for token in clean_and_tokenize(term)])\n",
    "    counter.update(res)\n",
    "    return counter.most_common(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most common terms by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    352\n",
      "1      get    346\n",
      "2      one    267\n",
      "3     time    232\n",
      "4      new    216\n",
      "5     love    209\n",
      "6       go    205\n",
      "7   people    197\n",
      "8     good    186\n",
      "9      day    186\n",
      "10    know    184\n",
      "11     see    168\n",
      "12     got    159\n",
      "13       2    159\n",
      "14    back    157\n",
      "15   think    157\n",
      "16   would    154\n",
      "17    best    152\n",
      "18     amp    152\n",
      "19    make    149\n",
      "20   still    144\n",
      "21    last    144\n",
      "22      __    137\n",
      "23    need    136\n",
      "24    want    135\n",
      "25       1    120\n",
      "26     way    118\n",
      "27  really    117\n",
      "28   great    116\n",
      "29     lol    113\n",
      "30    game    113\n",
      "31    year    110\n",
      "32    well    109\n",
      "33     via    108\n",
      "34   going    108\n",
      "35       u    107\n",
      "36   world    103\n",
      "37      im    102\n",
      "38    much    102\n",
      "39   first    101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Female Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    451\n",
      "1      get    353\n",
      "2       __    333\n",
      "3      one    332\n",
      "4     love    315\n",
      "5      day    296\n",
      "6       go    278\n",
      "7   people    249\n",
      "8     time    241\n",
      "9     know    210\n",
      "10     new    205\n",
      "11     amp    203\n",
      "12    want    199\n",
      "13     got    190\n",
      "14    best    189\n",
      "15    last    179\n",
      "16    back    172\n",
      "17    good    167\n",
      "18    make    164\n",
      "19     see    162\n",
      "20       2    161\n",
      "21   still    160\n",
      "22     ___    159\n",
      "23    need    157\n",
      "24   today    144\n",
      "25       u    143\n",
      "26    life    140\n",
      "27   makes    136\n",
      "28   going    134\n",
      "29   think    134\n",
      "30  really    132\n",
      "31    even    131\n",
      "32   right    130\n",
      "33   would    130\n",
      "34      im    130\n",
      "35     lol    124\n",
      "36    work    121\n",
      "37    much    119\n",
      "38  always    117\n",
      "39   never    116\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Brand Most Common Terms\n",
      "\n",
      "       Term  Count\n",
      "0   weather   2279\n",
      "1       get   1326\n",
      "2   channel   1169\n",
      "3        15   1166\n",
      "4   updates   1147\n",
      "5        40    727\n",
      "6        39    424\n",
      "7       new    244\n",
      "8       amp    191\n",
      "9        us    167\n",
      "10       10    163\n",
      "11      one    156\n",
      "12     like    153\n",
      "13     love    139\n",
      "14      see    139\n",
      "15        2    125\n",
      "16     last    118\n",
      "17     best    116\n",
      "18      via    114\n",
      "19      day    110\n",
      "20     time    110\n",
      "21     year    109\n",
      "22    check     97\n",
      "23     week     96\n",
      "24        5     93\n",
      "25   people     93\n",
      "26     back     92\n",
      "27        3     91\n",
      "28   thanks     88\n",
      "29        1     87\n",
      "30     look     86\n",
      "31    first     86\n",
      "32     2015     86\n",
      "33     make     84\n",
      "34    great     80\n",
      "35       07     80\n",
      "36     know     79\n",
      "37     game     79\n",
      "38     come     78\n",
      "39    world     78\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_amount = 50\n",
    "\n",
    "for gender in genders:\n",
    "    gender_most_common_terms=get_gender_most_common_terms(loaded_tweets.loc[loaded_tweets['gender'] == gender]['text'],top_amount)\n",
    "    gender_most_common_terms=pd.DataFrame(gender_most_common_terms, columns=['Term', 'Count'])\n",
    "    print(str.upper(gender[0])+ gender[1:]+ ' Most Common Terms\\n')\n",
    "    print(gender_most_common_terms)\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "import keras.preprocessing.text as kpt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[''.join([token + \" \" for token in tweet]) \n",
    "           for tweet in tokenized_tweets]\n",
    "\n",
    "test_ratio=0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_list, \n",
    "                                                    gender_column, \n",
    "                                                    test_size=test_ratio, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "term_cap = 5000\n",
    "tokenizer = Tokenizer(num_words=term_cap)\n",
    "tokenizer.fit_on_texts(text_list)\n",
    "term_dictionary= tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(term_dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(txt):\n",
    "    return [term_dictionary[word] \n",
    "            for word in kpt.text_to_word_sequence(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_indices_train = [words_to_indices(text) for text in X_train]\n",
    "words_to_indices_test=[words_to_indices(text) for text in X_test]\n",
    "\n",
    "words_to_indices_train = np.asarray(words_to_indices_train)\n",
    "words_to_indices_test = np.asarray(words_to_indices_test)\n",
    "\n",
    "X_train_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_train, mode='binary')\n",
    "X_test_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_test, mode='binary')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical_matrix = keras.utils.to_categorical(y_train_encoded, 3)\n",
    "y_test_categorical_matrix = keras.utils.to_categorical(y_test_encoded, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To prevent overfitting, we're using Dropout on hidden layers\n",
    "For more information, please refer to this article:\n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_1 (Dense)              (None, 2500)              12502500  \n",
      "_________________________________________________________________\n",
      "layer_2 (Dropout)            (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "layer_3 (Dense)              (None, 1250)              3126250   \n",
      "_________________________________________________________________\n",
      "layer_4 (Dropout)            (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "layer_5 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 15,632,503\n",
      "Trainable params: 15,632,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(int(term_cap/2), input_shape=(term_cap,), activation='linear', name='layer_1'))\n",
    "model.add(Dropout(0.5, name='layer_2'))\n",
    "model.add(Dense(int(term_cap/4), activation='sigmoid', name='layer_3'))\n",
    "model.add(Dropout(0.5, name='layer_4'))\n",
    "model.add(Dense(len(genders), activation='softmax', name='layer_5'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Keras\n",
      "--------------------\n",
      "Train on 15068 samples, validate on 3768 samples\n",
      "Epoch 1/3\n",
      " 3200/15068 [=====>........................] - ETA: 2:54 - loss: 1.0716 - acc: 0.5100"
     ]
    }
   ],
   "source": [
    "print(\"Training Keras\")\n",
    "print('-'*20)\n",
    "\n",
    "batch = 32\n",
    "num_iterations = 3\n",
    "visible = 1\n",
    "\n",
    "model.fit(X_train_tokenized_matrix, y_train_categorical_matrix,\n",
    "  batch_size = batch,\n",
    "  epochs = num_iterations,\n",
    "  verbose = visible,\n",
    "  validation_data = (X_test_tokenized_matrix, y_test_categorical_matrix), shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Keras\")\n",
    "scores = model.evaluate(X_test_tokenized_matrix, y_test_categorical_matrix, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Params: batch_size = \"+str(batch)+\"\\tepochs = \"+str(num_iterations))\n",
    "print(\"Model Accuracy %.2f\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model.json', 'w') as json_file:\n",
    "#    json_file.write(model.to_json())\n",
    "#weights_file_name = 'modelWeights'\n",
    "#model.save_weights(weights_file_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers: SVN and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from time import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier train and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier,classifier_name, X_train, X_test):\n",
    "    #Train\n",
    "    print(\"Training \"+classifier_name)\n",
    "    print('-'*20)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Test \n",
    "    print(\"Testing \"+classifier_name)\n",
    "    pred = classifier.predict(X_test)\n",
    "\n",
    "    #Accuracy Evaluation\n",
    "    accuracy = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "    print(\"Accuracy: %.2f\" % accuracy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tfidf = []\n",
    "classifiers = [(SGDClassifier(), \"SVM\"),(KNeighborsClassifier(), \"K-Nearest Neighbors\")]\n",
    "\n",
    "for classifier, classifier_name in classifiers:\n",
    "    test_classifier(classifier,classifier_name, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most common country - US & Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_location_train, tweet_location_test = train_test_split(clean_loaded_tweets[\"user_timezone\"], \n",
    "                                                           test_size=0.30, \n",
    "                                                           shuffle = False)\n",
    "tweet_location_train = tweet_location_train.dropna()\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(tweet_location_train)\n",
    "print(counter.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(consumer_key,\n",
    "                 consumer_secret, \n",
    "                 access_token, \n",
    "                 access_secret):\n",
    "    \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = authenticate('xGmH3k9cREZ4hpnjsYQtIQ5um',\n",
    "                  '25bBLLue9d9LT8RySWTTG4dwvn0iEbymwpvgfvUtsopghkLVrL',\n",
    "                  '2827486724-433UvU84r38VPaAbmqyzT95MvjHUIcB25NUZYQ7',\n",
    "                  'GuGv8nwmVEv8u3ItlpYNwxoO5HOzAseic6RlgETXffd71')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_name = 'tweetsFromAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saves the tweeets to file by batch downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(TweetStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 15000:\n",
    "                try:\n",
    "                    with open(tweets_file_name+'.json', 'a') as tweet_file:\n",
    "                        tweet_file.write(data)\n",
    "                        self.num_tweets += 1\n",
    "                        return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_data: \" + str(e))\n",
    "                    return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"Error: \"+status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_listener = TweetStreamListener()\n",
    "#tweet_stream = tweepy.Stream(auth=api.auth, listener=tweet_listener)\n",
    "#tweet_stream.filter(locations=[-162.8,28.2,-64.4,71.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and pre-process tweets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweet_file = open(tweets_file_name+'.json', \"r\")\n",
    "with open(tweets_file_name + '.json') as tweet_file:\n",
    "    proccessed_tweets = []\n",
    "    for jsonStr in tweet_file:\n",
    "        try:\n",
    "            tweet = json.loads(jsonStr)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        tweet_text = tweet['text']\n",
    "        #print(\"Text: \"+tweet_text)\n",
    "        tweet_id = tweet['id']\n",
    "        #print(\"ID: \"+str(tweet_id))\n",
    "            \n",
    "        cleaned_tweet_text = \"\"\n",
    "        for token in clean_and_tokenize(tweet_text):\n",
    "            cleaned_tweet_text += token + \" \"\n",
    "            \n",
    "        #print(cleaned_tweet_text)\n",
    "        #print('-'*10)['Tweet_ID','Tweet_Text']\n",
    "        proccessed_tweets.append({'Tweet_ID': tweet_id, 'Tweet_Text': cleaned_tweet_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proccessed_tweets = pd.DataFrame(proccessed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(proccessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Proccess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loaded_tweet in loaded_tweets:\n",
    "#    text = \"\"\n",
    "#    for token in clean_and_tokenize(loaded_tweet):\n",
    "#        text += token + \" \"\n",
    "#    proccessed_tweets.append(text)\n",
    "\n",
    "#print(proccessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get term count from downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = get_gender_most_common_terms(proccessed_tweets['Tweet_Text'], top_amount)\n",
    "term_df = pd.DataFrame(data, columns = ['Term','Frequency'])\n",
    "\n",
    "print(term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=term_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToArr(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in term_dictionary:\n",
    "            wordIndices.append(term_dictionary[word])\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the gender of the downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = ['brand', 'female', 'male']\n",
    "prediction_df = []#pd.DataFrame(columns=['Tweet ID','Predicted Gender'])\n",
    "\n",
    "for id in proccessed_tweets['Tweet_ID']:\n",
    "    tweet_text = proccessed_tweets.loc[proccessed_tweets['Tweet_ID'] == id]['Tweet_Text']\n",
    "    tweet_text = tweet_text.to_string()\n",
    "    #print(type(tweet_text))\n",
    "    testArr = textToArr(tweet_text)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    pred = model.predict(input)\n",
    "    data = tweet_text + \",\" + str(genders[np.argmax(pred)]) + \",\" + str(pred[0][np.argmax(pred)] * 100) + '\\n'\n",
    "    pred_gender= str(genders[np.argmax(pred)])\n",
    "    #row = \n",
    "    #print(row)\n",
    "    prediction_df.append([id,pred_gender])#[id]= pred_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(prediction_df)\n",
    "#print('-'*20)\n",
    "prediction_df=pd.DataFrame(prediction_df, columns = ['id', 'gender'])\n",
    "#print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genderize import Genderize\n",
    "\n",
    "tweet_file = open(tweets_file_name+'.json', \"r\")\n",
    "downloaded_tweets_id_name = dict()\n",
    "#a=0\n",
    "for jsonStr in tweet_file:\n",
    "    try:\n",
    "        tweet = json.loads(jsonStr)\n",
    "        downloaded_tweets_id_name[tweet['id']]=tweet['user']['name']\n",
    "        #a=a+1\n",
    "        #if a> 1000:\n",
    "            #break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = set([downloaded_tweets_id_name[id].split()[0] for id in downloaded_tweets_id_name])\n",
    "#print(downloaded_tweets_id_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To determine the accuracy of our model on the downloaded tweets, we compare our predicted gender to a prediction of another source - the Genderize API - more on the API at https://genderize.io/\n",
    "This API is throwing an exception if the request rate is too high, so to avoid technical problems we downloaded the most accurate predicitons (above 80%) to a csv.\n",
    "The purpose of using this data is to compare our data to our models predictions and help us determine its accuracy for predicting the gender for live tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names set, in order to use only distinct values\n",
    "genderizer = Genderize(timeout = 5.0)\n",
    "\n",
    "path_to_genderized_file = 'name_gender.csv'\n",
    "try:\n",
    "    names_gender = genderizer.get(names)\n",
    "    names_gender.to_csv(path_to_genderized_file)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    names_gender = pd.read_csv(path_to_genderized_file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(names_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "total = 0\n",
    "for tweet_id in downloaded_tweets_id_name:\n",
    "    tweet_name = downloaded_tweets_id_name[tweet_id].split()[0]\n",
    "    genderized_gender = names_gender.loc[names_gender['name'] == tweet_name]['gender']\n",
    "    if not genderized_gender.empty:\n",
    "        total += 1\n",
    "        genderized_gender = genderized_gender.max()\n",
    "        predicted_gender = prediction_df.loc[prediction_df['id'] == tweet_id]['gender'].max()\n",
    "\n",
    "        if genderized_gender == predicted_gender:\n",
    "            true_positive += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = true_positive/total\n",
    "print(\"Accuracy: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
