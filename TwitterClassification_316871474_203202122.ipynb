{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"gender-classifier-DFE-791531.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tweets = pd.read_csv(path_to_file, encoding='latin-1')\n",
    "\n",
    "genders = {'female', 'male', 'brand'}\n",
    "clean_loaded_tweets = loaded_tweets.loc[loaded_tweets['gender'].isin(genders)]\n",
    "\n",
    "clean_loaded_tweets.loc[loaded_tweets['text'].isnull(), 'text'] = \"-\"\n",
    "clean_loaded_tweets.loc[loaded_tweets['description'].isnull(), 'description'] = \"-\"\n",
    "\n",
    "tweets = clean_loaded_tweets[\"text\"].map(str) + \" \" + clean_loaded_tweets[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loaded Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female     6700\n",
      "male       6194\n",
      "brand      5942\n",
      "unknown    1117\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaned Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female    6700\n",
      "male      6194\n",
      "brand     5942\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(clean_loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Distribution By Gender\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_column = clean_loaded_tweets[\"gender\"]\n",
    "counter = Counter(gender_column)\n",
    "genders = counter.keys()\n",
    "tweet_amount = counter.values()\n",
    "indexes = np.arange(len(genders))\n",
    "\n",
    "plt.bar(indexes, tweet_amount, 0.5)\n",
    "plt.xticks(indexes + 0.5 * 0.5, genders)\n",
    "print('Tweet Distribution By Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop_words = set(stopwords.words('english') + punctuation + list(\"__\"))\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    tokenized_clean_text = []\n",
    "    text = re.sub(r'[^\\x00-\\x7f]*', r'', text)\n",
    "    tokens = tokens_re.findall(text)\n",
    "    tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        tokenized_clean_text.append(token)\n",
    "    return tokenized_clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = []\n",
    "for tweet in tweets:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweet = clean_and_tokenize(tweet)\n",
    "        tokenized_tweets.append(tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_most_common_terms(terms,top):\n",
    "    res = []\n",
    "    counter = Counter()\n",
    "    for term in terms:\n",
    "        res.extend([token for token in clean_and_tokenize(term)])\n",
    "    counter.update(res)\n",
    "    return counter.most_common(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most common terms by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    352\n",
      "1      get    346\n",
      "2      one    267\n",
      "3     time    232\n",
      "4      new    216\n",
      "5     love    209\n",
      "6       go    205\n",
      "7   people    197\n",
      "8     good    186\n",
      "9      day    186\n",
      "10    know    184\n",
      "11     see    168\n",
      "12     got    159\n",
      "13       2    159\n",
      "14    back    157\n",
      "15   think    157\n",
      "16   would    154\n",
      "17    best    152\n",
      "18     amp    152\n",
      "19    make    149\n",
      "20   still    144\n",
      "21    last    144\n",
      "22      __    137\n",
      "23    need    136\n",
      "24    want    135\n",
      "25       1    120\n",
      "26     way    118\n",
      "27  really    117\n",
      "28   great    116\n",
      "29     lol    113\n",
      "30    game    113\n",
      "31    year    110\n",
      "32    well    109\n",
      "33     via    108\n",
      "34   going    108\n",
      "35       u    107\n",
      "36   world    103\n",
      "37      im    102\n",
      "38    much    102\n",
      "39   first    101\n",
      "40   today    101\n",
      "41     man     99\n",
      "42  always     98\n",
      "43    come     97\n",
      "44     let     95\n",
      "45    even     94\n",
      "46    life     94\n",
      "47       3     93\n",
      "48    shit     93\n",
      "49   could     91\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Female Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    451\n",
      "1      get    353\n",
      "2       __    333\n",
      "3      one    332\n",
      "4     love    315\n",
      "5      day    296\n",
      "6       go    278\n",
      "7   people    249\n",
      "8     time    241\n",
      "9     know    210\n",
      "10     new    205\n",
      "11     amp    203\n",
      "12    want    199\n",
      "13     got    190\n",
      "14    best    189\n",
      "15    last    179\n",
      "16    back    172\n",
      "17    good    167\n",
      "18    make    164\n",
      "19     see    162\n",
      "20       2    161\n",
      "21   still    160\n",
      "22     ___    159\n",
      "23    need    157\n",
      "24   today    144\n",
      "25       u    143\n",
      "26    life    140\n",
      "27   makes    136\n",
      "28   going    134\n",
      "29   think    134\n",
      "30  really    132\n",
      "31    even    131\n",
      "32   right    130\n",
      "33   would    130\n",
      "34      im    130\n",
      "35     lol    124\n",
      "36    work    121\n",
      "37    much    119\n",
      "38  always    117\n",
      "39   never    116\n",
      "40    ever    113\n",
      "41   thank    108\n",
      "42     way    108\n",
      "43   world    106\n",
      "44    take    106\n",
      "45   thing    105\n",
      "46   happy    105\n",
      "47    girl    104\n",
      "48    shit    103\n",
      "49     let    102\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Brand Most Common Terms\n",
      "\n",
      "                   Term  Count\n",
      "0               weather   2279\n",
      "1                   get   1326\n",
      "2               channel   1169\n",
      "3                    15   1166\n",
      "4               updates   1147\n",
      "5                    40    727\n",
      "6                    39    424\n",
      "7                   new    244\n",
      "8                   amp    191\n",
      "9                    us    167\n",
      "10                   10    163\n",
      "11                  one    156\n",
      "12                 like    153\n",
      "13                 love    139\n",
      "14                  see    139\n",
      "15                    2    125\n",
      "16                 last    118\n",
      "17                 best    116\n",
      "18                  via    114\n",
      "19                  day    110\n",
      "20                 time    110\n",
      "21                 year    109\n",
      "22                check     97\n",
      "23                 week     96\n",
      "24                    5     93\n",
      "25               people     93\n",
      "26                 back     92\n",
      "27                    3     91\n",
      "28               thanks     88\n",
      "29                    1     87\n",
      "30                 look     86\n",
      "31                first     86\n",
      "32                 2015     86\n",
      "33                 make     84\n",
      "34                great     80\n",
      "35                   07     80\n",
      "36                 know     79\n",
      "37                 game     79\n",
      "38                 come     78\n",
      "39                world     78\n",
      "40                makes     77\n",
      "41                   gt     76\n",
      "42                today     75\n",
      "43                   00     75\n",
      "44                   06     75\n",
      "45                video     74\n",
      "46                 good     72\n",
      "47  #pushawardslizquens     72\n",
      "48                 find     71\n",
      "49            halloween     71\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_amount = 50\n",
    "\n",
    "for gender in genders:\n",
    "    gender_most_common_terms=get_gender_most_common_terms(\n",
    "        loaded_tweets.loc[loaded_tweets['gender'] == gender]['text'],\n",
    "        top_amount)\n",
    "    \n",
    "    gender_most_common_terms=pd.DataFrame(gender_most_common_terms, columns=['Term', 'Count'])\n",
    "    print(str.upper(gender[0])+ gender[1:]+ ' Most Common Terms\\n')\n",
    "    print(gender_most_common_terms)\n",
    "    print(\"\\n\"+'-'*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "import keras.preprocessing.text as kerasPreprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[''.join([token + \" \" for token in tweet]) \n",
    "           for tweet in tokenized_tweets]\n",
    "\n",
    "test_ratio=0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_list, \n",
    "                                                    gender_column, \n",
    "                                                    test_size=test_ratio, \n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "For our models we've chosen Keras, SVN and K-Nearest Neighbors which all use the TF-IDF Vectorizer. For this reason, we're saving the most common words in a dictionary to refer to when converting from text and/or description from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "term_cap = 5000\n",
    "tokenizer = Tokenizer(num_words=term_cap)\n",
    "tokenizer.fit_on_texts(text_list)\n",
    "term_dictionary= tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(txt):\n",
    "    return [term_dictionary[word] \n",
    "            for word in kerasPreprocessing.text_to_word_sequence(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_indices_train = [words_to_indices(text) for text in X_train]\n",
    "words_to_indices_train = np.asarray(words_to_indices_train)\n",
    "\n",
    "words_to_indices_test=[words_to_indices(text) for text in X_test]\n",
    "words_to_indices_test = np.asarray(words_to_indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the Sequential model whih is a linear stack of layers.\n",
    "for more information: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To prevent overfitting, we're using Dropout on hidden layers\n",
    "For more information, please refer to this article:\n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(1024, input_shape=(term_cap,), name='layer_1'),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5, name='layer_2'),\n",
    "    Dense(512, name='layer_3'),\n",
    "    Activation('sigmoid'),\n",
    "    Dropout(0.5, name='layer_4'),\n",
    "    Dense(len(genders), name='layer_5'),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_train, mode='binary')\n",
    "X_test_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_test, mode='binary')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "y_train_categorical_matrix = keras.utils.to_categorical(y_train_encoded, 3)\n",
    "y_test_categorical_matrix = keras.utils.to_categorical(y_test_encoded, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Keras\n",
      "--------------------\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15068 samples, validate on 3768 samples\n",
      "Epoch 1/2\n",
      "15068/15068 [==============================] - 18s 1ms/step - loss: 1.0037 - acc: 0.4995 - val_loss: 0.8775 - val_acc: 0.5982\n",
      "Epoch 2/2\n",
      "15068/15068 [==============================] - 17s 1ms/step - loss: 0.7012 - acc: 0.6848 - val_loss: 0.8983 - val_acc: 0.6096\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Keras\")\n",
    "print('-'*20)\n",
    "\n",
    "batch = 256\n",
    "num_iterations = 2\n",
    "visible = 1\n",
    "\n",
    "model.fit(X_train_tokenized_matrix, y_train_categorical_matrix,\n",
    "  batch_size = batch,\n",
    "  epochs = num_iterations,\n",
    "  verbose = visible,\n",
    "  validation_data = (X_test_tokenized_matrix, y_test_categorical_matrix), shuffle = True)\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Keras\n",
      "3768/3768 [==============================] - 2s 498us/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Keras\")\n",
    "scores = model.evaluate(X_test_tokenized_matrix, y_test_categorical_matrix, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: batch_size = 256\tepochs = 2\n",
      "Model Accuracy 60.96%\n"
     ]
    }
   ],
   "source": [
    "print(\"Params: batch_size = \"+str(batch)+\"\\tepochs = \"+str(num_iterations))\n",
    "print(\"Model Accuracy %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVN and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from time import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier train and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier,classifier_name, X_train, X_test):\n",
    "    #Train\n",
    "    print(\"Training \"+classifier_name)\n",
    "    print('-'*20)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Test \n",
    "    print(\"Testing \"+classifier_name)\n",
    "    pred = classifier.predict(X_test)\n",
    "\n",
    "    #Accuracy Evaluation\n",
    "    accuracy = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "--------------------\n",
      "Testing SVM\n",
      "Accuracy: 60.30%\n",
      "\n",
      "Training K-Nearest Neighbors\n",
      "--------------------\n",
      "Testing K-Nearest Neighbors\n",
      "Accuracy: 49.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_tfidf = []\n",
    "classifiers = [(SGDClassifier(), \"SVM\"),(KNeighborsClassifier(), \"K-Nearest Neighbors\")]\n",
    "\n",
    "for classifier, classifier_name in classifiers:\n",
    "    test_classifier(classifier,classifier_name, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most common country - US & Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Eastern Time (US & Canada)', 1667)]\n"
     ]
    }
   ],
   "source": [
    "tweet_location_train, tweet_location_test = train_test_split(clean_loaded_tweets[\"user_timezone\"], \n",
    "                                                           test_size=0.30, \n",
    "                                                           shuffle = False)\n",
    "tweet_location_train = tweet_location_train.dropna()\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(tweet_location_train)\n",
    "print(counter.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(consumer_key,\n",
    "                 consumer_secret, \n",
    "                 access_token, \n",
    "                 access_secret):\n",
    "    \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = authenticate('xGmH3k9cREZ4hpnjsYQtIQ5um',\n",
    "                  '25bBLLue9d9LT8RySWTTG4dwvn0iEbymwpvgfvUtsopghkLVrL',\n",
    "                  '2827486724-433UvU84r38VPaAbmqyzT95MvjHUIcB25NUZYQ7',\n",
    "                  'GuGv8nwmVEv8u3ItlpYNwxoO5HOzAseic6RlgETXffd71')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_name = 'downloaded_tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the tweeets to file by batch downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(TweetStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 15000:\n",
    "                try:\n",
    "                    with open(tweets_file_name+'.json', 'a') as tweet_file:\n",
    "                        tweet_file.write(data)\n",
    "                        self.num_tweets += 1\n",
    "                        return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_data: \" + str(e))\n",
    "                    return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"Error: \"+status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download live tweets with out custom StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_listener = TweetStreamListener()\n",
    "tweet_stream = tweepy.Stream(auth=api.auth, listener=tweet_listener)\n",
    "tweet_stream.filter(locations=[-162.8,28.2,-64.4,71.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and pre-process tweets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(tweets_file_name + '.json') as tweet_file:\n",
    "    proccessed_tweets = []\n",
    "    for jsonStr in tweet_file:\n",
    "        try:\n",
    "            tweet = json.loads(jsonStr)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        tweet_text = tweet['text']\n",
    "        tweet_id = tweet['id']\n",
    "            \n",
    "        cleaned_tweet_text = \"\"\n",
    "        for token in clean_and_tokenize(tweet_text):\n",
    "            cleaned_tweet_text += token + \" \"\n",
    "            \n",
    "        proccessed_tweets.append({'Tweet_ID': tweet_id, 'Tweet_Text': cleaned_tweet_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proccessed_tweets = pd.DataFrame(proccessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get term count from downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Term  Count\n",
      "0               like    820\n",
      "1                job    704\n",
      "2                day    603\n",
      "3                 im    522\n",
      "4                 st    456\n",
      "5                see    443\n",
      "6                get    414\n",
      "7               time    388\n",
      "8              happy    382\n",
      "9                one    363\n",
      "10             today    350\n",
      "11              love    347\n",
      "12                go    326\n",
      "13             great    320\n",
      "14              link    316\n",
      "15               amp    314\n",
      "16              dont    308\n",
      "17             click    304\n",
      "18               new    302\n",
      "19              good    300\n",
      "20               bio    297\n",
      "21              want    276\n",
      "22               got    269\n",
      "23              work    261\n",
      "24              know    253\n",
      "25  @realdonaldtrump    238\n",
      "26            people    236\n",
      "27              back    231\n",
      "28               lol    229\n",
      "29             apply    227\n",
      "30            latest    223\n",
      "31                us    218\n",
      "32            really    216\n",
      "33              team    213\n",
      "34                 2    204\n",
      "35              need    202\n",
      "36                 3    201\n",
      "37                 1    197\n",
      "38            hiring    192\n",
      "39             would    191\n",
      "40             right    189\n",
      "41             think    189\n",
      "42              game    181\n",
      "43              last    181\n",
      "44             going    180\n",
      "45              shit    172\n",
      "46             thank    171\n",
      "47             still    170\n",
      "48              much    169\n",
      "49             first    164\n"
     ]
    }
   ],
   "source": [
    "data = get_gender_most_common_terms(proccessed_tweets['Tweet_Text'], top_amount)\n",
    "term_df = pd.DataFrame(data, columns = ['Term','Count'])\n",
    "\n",
    "print(term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=term_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the gender of the downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = ['brand', 'female', 'male']\n",
    "prediction_df = []\n",
    "\n",
    "for tweet_id in proccessed_tweets['Tweet_ID']:\n",
    "    tweet_text = proccessed_tweets.loc[proccessed_tweets['Tweet_ID'] == tweet_id]['Tweet_Text']\n",
    "    tweet_text = tweet_text.to_string()\n",
    "    \n",
    "    words = kerasPreprocessing.text_to_word_sequence(tweet_text)\n",
    "    indices = []\n",
    "    for term in words:\n",
    "        if term in term_dictionary:\n",
    "            indices.append(term_dictionary[term])\n",
    "            \n",
    "    model_matrix = tokenizer.sequences_to_matrix([indices], mode='binary')\n",
    "    \n",
    "    pred = model.predict(model_matrix)\n",
    "    data = tweet_text + \",\" + str(genders[np.argmax(pred)]) + \",\" + str(pred[0][np.argmax(pred)] * 100) + '\\n'\n",
    "    pred_gender= str(genders[np.argmax(pred)])\n",
    "    \n",
    "    prediction_df.append([tweet_id,pred_gender])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df=pd.DataFrame(prediction_df, columns = ['id', 'gender'])\n",
    "prediction_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genderize import Genderize\n",
    "\n",
    "with open(tweets_file_name+'.json', \"r\") as tweet_file:\n",
    "    downloaded_tweets_id_name = dict()\n",
    "    for jsonStr in tweet_file:\n",
    "        try:\n",
    "            tweet = json.loads(jsonStr)\n",
    "            downloaded_tweets_id_name[tweet['id']]=tweet['user']['name']\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To determine the accuracy of our model on the downloaded tweets, we compare our predicted gender to a prediction of another source - the Genderize API - more on the API at https://genderize.io/\n",
    "This API is throwing an exception if the request rate is too high, so to avoid technical problems we downloaded the most accurate predicitons (above 80%) to a csv.\n",
    "The purpose of using this data is to compare our data to our models predictions and help us determine its accuracy for predicting the gender for live tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names set, in order to use only distinct values\n",
    "names = set([downloaded_tweets_id_name[id].split()[0] for id in downloaded_tweets_id_name])\n",
    "genderizer = Genderize(timeout = 5.0)\n",
    "\n",
    "path_to_genderized_file = 'name_gender.csv'\n",
    "try:\n",
    "    names_gender = genderizer.get(names)\n",
    "    names_gender.to_csv(path_to_genderized_file)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    names_gender = pd.read_csv(path_to_genderized_file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = prediction_df.loc[prediction_df['gender'].isin({'male', 'female'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "total = 0\n",
    "\n",
    "for tweet_id in downloaded_tweets_id_name:\n",
    "    tweet_name = downloaded_tweets_id_name[tweet_id].split()[0]\n",
    "    genderized_gender = names_gender.loc[names_gender['name'] == tweet_name]['gender']\n",
    "    if not genderized_gender.empty:\n",
    "        total += 1\n",
    "        genderized_gender = genderized_gender.max()\n",
    "        predicted_gender = prediction_df.loc[prediction_df['id'] == tweet_id]['gender'].max()\n",
    "\n",
    "        if genderized_gender == predicted_gender:\n",
    "            true_positive += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = true_positive/total\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
