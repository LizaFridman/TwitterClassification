{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"gender-classifier-DFE-791531.csv\"\n",
    "\n",
    "loaded_tweets = pd.read_csv(path_to_file, encoding='latin-1')\n",
    "\n",
    "genders = {'female', 'male', 'brand'}\n",
    "clean_loaded_tweets = loaded_tweets.loc[loaded_tweets['gender'].isin(genders)]\n",
    "\n",
    "clean_loaded_tweets.loc[loaded_tweets['text'].isnull(), 'text'] = \"-\"\n",
    "clean_loaded_tweets.loc[loaded_tweets['description'].isnull(), 'description'] = \"-\"\n",
    "\n",
    "tweets = clean_loaded_tweets[\"text\"].map(str) + \" \" + clean_loaded_tweets[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loaded Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female     6700\n",
      "male       6194\n",
      "brand      5942\n",
      "unknown    1117\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaned loaded Tweets by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female    6700\n",
      "male      6194\n",
      "brand     5942\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(clean_loaded_tweets[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop_words = set(stopwords.words('english') + punctuation + list(\"__\"))\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenized_clean_text = []\n",
    "    text = re.sub(r'[^\\x00-\\x7f]*', r'', text)\n",
    "    tokens = tokens_re.findall(text)\n",
    "    tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        tokenized_clean_text.append(token)\n",
    "    return tokenized_clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if isinstance(tweet, str):\n",
    "        tokenized_tweet = tokenize(tweet)\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "        \n",
    "gender_column = clean_loaded_tweets[\"gender\"]\n",
    "counter = Counter(gender_column)\n",
    "genders = counter.keys()\n",
    "tweet_amount = counter.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Distributions By Gender\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes = np.arange(len(genders))\n",
    "plt.bar(indexes, tweet_amount, 0.5)\n",
    "plt.xticks(indexes + 0.5 * 0.5, genders)\n",
    "print('Tweet Distributions By Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_most_common_terms(terms,top):\n",
    "    res = []\n",
    "    counter = Counter()\n",
    "    for term in terms:\n",
    "        res.extend([token for token in tokenize(term)])\n",
    "    counter.update(res)\n",
    "    return counter.most_common(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most common terms by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    352\n",
      "1      get    346\n",
      "2      one    267\n",
      "3     time    232\n",
      "4      new    216\n",
      "5     love    209\n",
      "6       go    205\n",
      "7   people    197\n",
      "8     good    186\n",
      "9      day    186\n",
      "10    know    184\n",
      "11     see    168\n",
      "12     got    159\n",
      "13       2    159\n",
      "14    back    157\n",
      "15   think    157\n",
      "16   would    154\n",
      "17    best    152\n",
      "18     amp    152\n",
      "19    make    149\n",
      "20   still    144\n",
      "21    last    144\n",
      "22      __    137\n",
      "23    need    136\n",
      "24    want    135\n",
      "25       1    120\n",
      "26     way    118\n",
      "27  really    117\n",
      "28   great    116\n",
      "29     lol    113\n",
      "30    game    113\n",
      "31    year    110\n",
      "32    well    109\n",
      "33     via    108\n",
      "34   going    108\n",
      "35       u    107\n",
      "36   world    103\n",
      "37      im    102\n",
      "38    much    102\n",
      "39   first    101\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Female Most Common Terms\n",
      "\n",
      "      Term  Count\n",
      "0     like    451\n",
      "1      get    353\n",
      "2       __    333\n",
      "3      one    332\n",
      "4     love    315\n",
      "5      day    296\n",
      "6       go    278\n",
      "7   people    249\n",
      "8     time    241\n",
      "9     know    210\n",
      "10     new    205\n",
      "11     amp    203\n",
      "12    want    199\n",
      "13     got    190\n",
      "14    best    189\n",
      "15    last    179\n",
      "16    back    172\n",
      "17    good    167\n",
      "18    make    164\n",
      "19     see    162\n",
      "20       2    161\n",
      "21   still    160\n",
      "22     ___    159\n",
      "23    need    157\n",
      "24   today    144\n",
      "25       u    143\n",
      "26    life    140\n",
      "27   makes    136\n",
      "28   going    134\n",
      "29   think    134\n",
      "30  really    132\n",
      "31    even    131\n",
      "32   right    130\n",
      "33   would    130\n",
      "34      im    130\n",
      "35     lol    124\n",
      "36    work    121\n",
      "37    much    119\n",
      "38  always    117\n",
      "39   never    116\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Brand Most Common Terms\n",
      "\n",
      "       Term  Count\n",
      "0   weather   2279\n",
      "1       get   1326\n",
      "2   channel   1169\n",
      "3        15   1166\n",
      "4   updates   1147\n",
      "5        40    727\n",
      "6        39    424\n",
      "7       new    244\n",
      "8       amp    191\n",
      "9        us    167\n",
      "10       10    163\n",
      "11      one    156\n",
      "12     like    153\n",
      "13     love    139\n",
      "14      see    139\n",
      "15        2    125\n",
      "16     last    118\n",
      "17     best    116\n",
      "18      via    114\n",
      "19      day    110\n",
      "20     time    110\n",
      "21     year    109\n",
      "22    check     97\n",
      "23     week     96\n",
      "24        5     93\n",
      "25   people     93\n",
      "26     back     92\n",
      "27        3     91\n",
      "28   thanks     88\n",
      "29        1     87\n",
      "30     look     86\n",
      "31    first     86\n",
      "32     2015     86\n",
      "33     make     84\n",
      "34    great     80\n",
      "35       07     80\n",
      "36     know     79\n",
      "37     game     79\n",
      "38     come     78\n",
      "39    world     78\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_amount = 40\n",
    "\n",
    "for gender in genders:\n",
    "    gender_most_common_terms=get_gender_most_common_terms(loaded_tweets.loc[loaded_tweets['gender'] == gender]['text'],top_amount)\n",
    "    gender_most_common_terms=pd.DataFrame(gender_most_common_terms, columns=['Term', 'Count'])\n",
    "    print(str.upper(gender[0])+ gender[1:]+ ' Most Common Terms\\n')\n",
    "    print(gender_most_common_terms)\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "import keras.preprocessing.text as kpt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[''.join([token + \" \" for token in tweet]) for tweet in tokenized_tweets ]\n",
    "test_ratio=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_list, gender_column, test_size=test_ratio, shuffle=False)\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "term_cap = 5000\n",
    "tokenizer = Tokenizer(num_words=term_cap)\n",
    "tokenizer.fit_on_texts(text_list)\n",
    "term_dictionary= tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(term_dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(txt):\n",
    "    return [term_dictionary[word] for word in kpt.text_to_word_sequence(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_indices_train = [words_to_indices(text) for text in X_train]\n",
    "words_to_indices_test=[words_to_indices(text) for text in X_test]\n",
    "\n",
    "words_to_indices_train = np.asarray(words_to_indices_train)\n",
    "words_to_indices_test = np.asarray(words_to_indices_test)\n",
    "\n",
    "X_train_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_train, mode='binary')\n",
    "X_test_tokenized_matrix = tokenizer.sequences_to_matrix(words_to_indices_test, mode='binary')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To prevent overfitting, we're using Dropout on hidden layers\n",
    "For more information, please refer to this article:\n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical_matrix = keras.utils.to_categorical(y_train_encoded, 3)\n",
    "y_test_categorical_matrix = keras.utils.to_categorical(y_test_encoded, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_1 (Dense)              (None, 2500)              12502500  \n",
      "_________________________________________________________________\n",
      "layer_2 (Dropout)            (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "layer_3 (Dense)              (None, 1250)              3126250   \n",
      "_________________________________________________________________\n",
      "layer_4 (Dropout)            (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "layer_5 (Dense)              (None, 3)                 3753      \n",
      "=================================================================\n",
      "Total params: 15,632,503\n",
      "Trainable params: 15,632,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(int(term_cap/2), input_shape=(term_cap,), activation='linear', name='layer_1'))\n",
    "model.add(Dropout(0.5, name='layer_2'))\n",
    "model.add(Dense(int(term_cap/4), activation='sigmoid', name='layer_3'))\n",
    "model.add(Dropout(0.5, name='layer_4'))\n",
    "model.add(Dense(len(genders), activation='softmax', name='layer_5'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Keras\n",
      "--------------------\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15068 samples, validate on 3768 samples\n",
      "Epoch 1/3\n",
      "15068/15068 [==============================] - 120s 8ms/step - loss: 0.9585 - acc: 0.5548 - val_loss: 0.9121 - val_acc: 0.5839\n",
      "Epoch 2/3\n",
      "15068/15068 [==============================] - 117s 8ms/step - loss: 0.7134 - acc: 0.6872 - val_loss: 0.9469 - val_acc: 0.5804\n",
      "Epoch 3/3\n",
      "15068/15068 [==============================] - 124s 8ms/step - loss: 0.6267 - acc: 0.7278 - val_loss: 0.9847 - val_acc: 0.5833\n",
      "Testing Keras\n",
      "3768/3768 [==============================] - 5s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Keras\")\n",
    "print('-'*20)\n",
    "\n",
    "batch = 64\n",
    "num_iterations = 3\n",
    "visible = 1\n",
    "\n",
    "model.fit(X_train_tokenized_matrix, y_train_categorical_matrix,\n",
    "  batch_size = batch,\n",
    "  epochs = num_iterations,\n",
    "  verbose = visible,\n",
    "  validation_data = (X_test_tokenized_matrix, y_test_categorical_matrix), shuffle = True)\n",
    "\n",
    "print(\"Testing Keras\")\n",
    "scores = model.evaluate(X_test_tokenized_matrix, y_test_categorical_matrix, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: batch_size = 64\tepochs = 3\n",
      "Model Accuracy 58.33\n"
     ]
    }
   ],
   "source": [
    "print(\"Params: batch_size = \"+str(batch)+\"\\tepochs = \"+str(num_iterations))\n",
    "print(\"Model Accuracy %.2f\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model.json', 'w') as json_file:\n",
    "#    json_file.write(model.to_json())\n",
    "weights_file_name = 'modelWeights'\n",
    "model.save_weights(weights_file_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers: SVN and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from time import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier train and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier,classifier_name, X_train, X_test):\n",
    "    #Train\n",
    "    print(\"Training \"+classifier_name)\n",
    "    print('-'*20)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Test \n",
    "    print(\"Testing \"+classifier_name)\n",
    "    pred = classifier.predict(X_test)\n",
    "\n",
    "    #Accuracy Evaluation\n",
    "    accuracy = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "    print(\"Accuracy: %.2f\" % accuracy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "--------------------\n",
      "Testing SVM\n",
      "Accuracy: 0.60\n",
      "\n",
      "Training K-Nearest Neighbors\n",
      "--------------------\n",
      "Testing K-Nearest Neighbors\n",
      "Accuracy: 0.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_tfidf = []\n",
    "classifiers = [(SGDClassifier(), \"SVM\"),(KNeighborsClassifier(), \"K-Nearest Neighbors\")]\n",
    "\n",
    "for classifier, classifier_name in classifiers:\n",
    "    test_classifier(classifier,classifier_name, X_train_vectorized, X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find most common country - US & Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Eastern Time (US & Canada)', 1667)]\n"
     ]
    }
   ],
   "source": [
    "tweet_location_train, tweet_location_test = train_test_split(clean_loaded_tweets[\"user_timezone\"], \n",
    "                                                           test_size=0.30, \n",
    "                                                           shuffle = False)\n",
    "tweet_location_train = tweet_location_train.dropna()\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(tweet_location_train)\n",
    "print(counter.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(consumer_key,\n",
    "                 consumer_secret, \n",
    "                 access_token, \n",
    "                 access_secret):\n",
    "    \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = authenticate('xGmH3k9cREZ4hpnjsYQtIQ5um',\n",
    "                  '25bBLLue9d9LT8RySWTTG4dwvn0iEbymwpvgfvUtsopghkLVrL',\n",
    "                  '2827486724-433UvU84r38VPaAbmqyzT95MvjHUIcB25NUZYQ7',\n",
    "                  'GuGv8nwmVEv8u3ItlpYNwxoO5HOzAseic6RlgETXffd71')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file_name = 'tweetsFromAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saves the tweeets to file by batch downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(TweetStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 15000:\n",
    "                try:\n",
    "                    with open(tweets_file_name+'.json', 'a') as tweet_file:\n",
    "                        tweet_file.write(data)\n",
    "                        self.num_tweets += 1\n",
    "                        return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_data: \" + str(e))\n",
    "                    return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"Error: \"+status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_listener = TweetStreamListener()\n",
    "#tweet_stream = tweepy.Stream(auth=api.auth, listener=tweet_listener)\n",
    "#tweet_stream.filter(locations=[-162.8,28.2,-64.4,71.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read tweets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_file = open(tweets_file_name+'.json', \"r\")\n",
    "\n",
    "loaded_tweets = []\n",
    "for jsonStr in tweet_file:\n",
    "    try:\n",
    "        tweet = json.loads(jsonStr)\n",
    "        loaded_tweets.append(tweet['text'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Proccess the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "proccessed_tweets = []\n",
    "for loaded_tweet in loaded_tweets:\n",
    "    text = \"\"\n",
    "    for token in tokenize(loaded_tweet):\n",
    "        text += token + \" \"\n",
    "    proccessed_tweets.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get term count from downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Term  Frequency\n",
      "0     like        929\n",
      "1      job        736\n",
      "2       im        563\n",
      "3      get        480\n",
      "4      see        480\n",
      "5     time        387\n",
      "6      one        363\n",
      "7      amp        360\n",
      "8      new        333\n",
      "9     link        331\n",
      "10      go        328\n",
      "11    good        325\n",
      "12   click        323\n",
      "13    love        322\n",
      "14   great        321\n",
      "15     day        321\n",
      "16    want        307\n",
      "17     bio        299\n",
      "18    know        297\n",
      "19  people        293\n",
      "20    work        293\n",
      "21     lol        288\n",
      "22     got        281\n",
      "23    dont        276\n",
      "24      us        268\n",
      "25   apply        259\n",
      "26   today        251\n",
      "27    back        229\n",
      "28  latest        225\n",
      "29   going        225\n",
      "30  really        223\n",
      "31       3        209\n",
      "32    need        208\n",
      "33   would        202\n",
      "34  hiring        193\n",
      "35   think        191\n",
      "36    make        190\n",
      "37    even        189\n",
      "38    jobs        188\n",
      "39       2        182\n"
     ]
    }
   ],
   "source": [
    "data = get_gender_most_common_terms(proccessed_tweets, top_amount)\n",
    "term_df = pd.DataFrame(data, columns = ['Term','Frequency'])\n",
    "\n",
    "print(term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=term_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToArr(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in term_dictionary:\n",
    "            wordIndices.append(term_dictionary[word])\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the gender of the downloaded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = ['brand', 'female', 'male']\n",
    "prediction_df=pd.DataFrame(columns=['Tweet ID','Predicted Gender'])\n",
    "index=0\n",
    "for tweet in proccessed_tweets:\n",
    "    testArr = textToArr(tweet)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    pred = model.predict(input)\n",
    "    data = tweet + \",\" + str(genders[np.argmax(pred)]) + \",\" + str(pred[0][np.argmax(pred)] * 100) + '\\n'\n",
    "    pred_gender= str(genders[np.argmax(pred)])\n",
    "    prediction_df = prediction_df.append({'Tweet ID': index, 'Predicted Gender': pred_gender}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tweet ID Predicted Gender\n",
      "0            0           female\n",
      "1            0           female\n",
      "2            0           female\n",
      "3            0            brand\n",
      "4            0           female\n",
      "5            0            brand\n",
      "6            0           female\n",
      "7            0           female\n",
      "8            0           female\n",
      "9            0           female\n",
      "10           0            brand\n",
      "11           0           female\n",
      "12           0           female\n",
      "13           0           female\n",
      "14           0             male\n",
      "15           0             male\n",
      "16           0           female\n",
      "17           0            brand\n",
      "18           0             male\n",
      "19           0            brand\n",
      "20           0           female\n",
      "21           0           female\n",
      "22           0           female\n",
      "23           0            brand\n",
      "24           0             male\n",
      "25           0             male\n",
      "26           0           female\n",
      "27           0             male\n",
      "28           0           female\n",
      "29           0           female\n",
      "...        ...              ...\n",
      "14970        0             male\n",
      "14971        0             male\n",
      "14972        0           female\n",
      "14973        0           female\n",
      "14974        0           female\n",
      "14975        0            brand\n",
      "14976        0             male\n",
      "14977        0             male\n",
      "14978        0             male\n",
      "14979        0             male\n",
      "14980        0           female\n",
      "14981        0           female\n",
      "14982        0           female\n",
      "14983        0           female\n",
      "14984        0           female\n",
      "14985        0             male\n",
      "14986        0           female\n",
      "14987        0           female\n",
      "14988        0           female\n",
      "14989        0            brand\n",
      "14990        0            brand\n",
      "14991        0           female\n",
      "14992        0            brand\n",
      "14993        0           female\n",
      "14994        0           female\n",
      "14995        0             male\n",
      "14996        0            brand\n",
      "14997        0           female\n",
      "14998        0             male\n",
      "14999        0           female\n",
      "\n",
      "[15000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    7687\n",
       "male      5255\n",
       "brand     2058\n",
       "Name: Predicted Gender, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df[\"Predicted Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genderize import Genderize\n",
    "\n",
    "tweet_file = open(tweets_file_name+'.json', \"r\")\n",
    "tweets_id_name_dict = dict()\n",
    "a=0\n",
    "for jsonStr in tweet_file:\n",
    "    try:\n",
    "        tweet = json.loads(jsonStr)\n",
    "        tweets_id_name_dict[tweet['id']]=tweet['user']['name']\n",
    "        a=a+1\n",
    "        if a> 1000:\n",
    "            break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = set([tweets_id_name_dict[id].split()[0] for id in tweets_id_name_dict])\n",
    "#print(tweets_id_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To determine the accuracy of our model on the downloaded tweets, we compare our predicted gender to a prediction of another source - the Genderize API - more on the API at https://genderize.io/\n",
    "This API is throwing an exception if the request rate is too high, so to avoid technical problems we downloaded the most accurate predicitons (above 80%) to a csv.\n",
    "The purpose of using this data is to compare our data to our models predictions and help us determine its accuracy for predicting the gender for live tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names set, in order to use only distinct values\n",
    "genderizer = Genderize(timeout = 5.0)\n",
    "\n",
    "path_to_genderized_file = 'name_gender.csv'\n",
    "try:\n",
    "    names_gender = genderizer.get(names)\n",
    "    names_gender.to_csv(path_to_genderized_file)\n",
    "except:\n",
    "    names_gender = pd.read_csv(path_to_genderized_file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     count  gender            name  probability\n",
      "0    381.0    male         Ernesto         1.00\n",
      "1   4440.0  female            mary         1.00\n",
      "2   2925.0    male            Jeff         1.00\n",
      "3    167.0    male            Gabe         0.97\n",
      "4      1.0  female         sodapop         1.00\n",
      "5      1.0  female            Mama         1.00\n",
      "6    576.0  female         Caitlin         1.00\n",
      "7    929.0    male            Carl         1.00\n",
      "8   3821.0    male            Jose         0.99\n",
      "9    611.0    male         Douglas         1.00\n",
      "10    16.0    male            Trev         1.00\n",
      "11     1.0  female             The         1.00\n",
      "12  2787.0  female          Monica         1.00\n",
      "13  1233.0  female             Jen         0.99\n",
      "14   954.0    male           Colin         0.99\n",
      "15     1.0  female        Jessmine         1.00\n",
      "16  1799.0  female         Shannon         0.93\n",
      "17  2262.0    male             Rob         0.99\n",
      "18    83.0    male          mohsen         1.00\n",
      "19   710.0    male            Drew         0.95\n",
      "20   836.0    male            HUGO         1.00\n",
      "21  1097.0    male          Philip         1.00\n",
      "22   172.0  female            Bree         1.00\n",
      "23   422.0    male          Camilo         0.99\n",
      "24    73.0  female           Raven         0.84\n",
      "25     4.0    male          Gavino         1.00\n",
      "26  2597.0    male            Josh         1.00\n",
      "27   721.0    male         Gregory         1.00\n",
      "28  1072.0    male            Erik         0.99\n",
      "29  1060.0  female             Pam         1.00\n",
      "30     2.0  female           every         1.00\n",
      "31  1816.0    male           Craig         1.00\n",
      "32  6717.0  female        Jennifer         1.00\n",
      "33     1.0    male             TMJ         1.00\n",
      "34  4915.0    male            Matt         1.00\n",
      "35   338.0    male          Marvin         1.00\n",
      "36  1812.0    male            Bill         1.00\n",
      "37  4110.0  female          Sandra         1.00\n",
      "38   912.0    male          Felipe         1.00\n",
      "39    57.0    male  Ø¹Ø¨Ø¯Ø§ÙÙÙ         1.00\n",
      "40   109.0    male              DA         0.83\n",
      "41  5168.0    male          Andrew         1.00\n",
      "42  6394.0  female            Lisa         1.00\n",
      "43  1309.0    male           Jacob         1.00\n",
      "44  5595.0    male            mike         1.00\n",
      "45  5595.0    male            Mike         1.00\n"
     ]
    }
   ],
   "source": [
    "print(names_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in tweets_id_name_dict:\n",
    "    tweet_name = tweets_id_name_dict[id].split()[0]\n",
    "    genderized_gender = names_gender.loc[names_gender['name'] == tweet_name]['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
